<!DOCTYPE html>
<html>

<head>
</head>

<style>
  body {
    background-color: black;
    text-align: left;
    color: darkseagreen;
    font-family: Gilroy,sans-serif;
  }
  h2 {
    color: ivory;
    font-family: Gilroy,sans-serif;
  }
</style>

  <body>
    <h2>Introduction to Latency</h2>
    <p>
    Although it may seem like magic, when you send data over a network from one point to another it is not instant.
    This is because we are limited to the speed of light, roughly 300 million meters per second. While that is quite fast,
    let's pretend we have a router in the DE-CIX New York internet exchange point (IXP) with an optical cable running perfectly to a router in Dubai, which is a total
    distance of ~11,000km. The fastest one bit of data in New York, manifested in reality most commonly as light sent through that optical fiber,
    can travel to the receiving photodiode of the optic in Dubai is ~36.7 ms due to the laws of physics. Realistically, it would be closer to ~40ms if you take the
    refractive index of fiber into account... more on that in the optical section.
    </p>
    <p><i>Note: A photodiode is a diode which produces current when it absorbs photons. The current is then converted into voltage with a transimpedance amflifer, and then to binary with a DAC.</i></p>
    <p>
    While this seems very fast (and it is compared to mailing a letter), we do not have perfect point to point links
    between all devices, servers, and nodes on the internet. That would not only be physically very messy but logistically
    impossible. For that reason, the internet is broken up into various sub-networks controlled by different businesses,
    organizations, governments, militaries, and even single individuals who manage and connect their networks to each other,
    often through service providers or various infrastructure providers.
    </p>
    <p>
    Since the design of peering multiple networks together was adopted, network traffic often passes through several
    networks, called Autonomous Systems (more on this in the BGP section), to reach it's destination. Let's look at an example!
    </p>
    <p><i>Example: Traffic to a SAAS Application</i></p>
    <p>
    Let's say you are a lucky work-from-home user in Tampa who needs to connect to your Microsoft Sharepoint folder online
    to access a Powerpoint you just got told you have to present tomorrow... yay. The network path you will likely
    take is out your local home modem/router to your local broadband provider, across upstream service providers, and finally into the Autonomous System hosting the application web server. Note that by this point we
    have likely already had our traffic routed through four or five different service providers upstream (on the way to target server). Once the
    traffic enters Microsoft's AS (Autonomous System), you likely will first hit a security device or two (Firewall, IDS/IPS), which will hand you off to a load balancer, then a front-end proxy server, which then
    routes your request to the appropriate web server. All in all, you have likely traveled some 13-15 "hops", which is just slang for separate network segment,
    to your target. But here's the catch... it also has to travel back to you!
    </p>

    <h2>One-Way vs Round-Trip Latency</h2>

    <p>
    Commonly, network operators, enthusiats, deliquents, and administrators misuse the term latency. Before I understood networking, I always
    associated latency, jitter, delay, lag, etc. as all the same thing: <i>slowness</i>. Latency explicitly means the time spent sending data from one hop to another,
    whether that be one or fifty hops away. The total time for the traffic to travel from the source to the destination is known as one-way latency.
    Therefore, the time it takes for network traffic to be sent to the target and receive a reply is the Round-Trip Latency.
    </p>

    <p>
    In our example above, each device the traffic passes through takes time, be it milliseconds or microseconds, to process your traffic
    and send it to the appropriate destination. This is known as processing delay and queing delay. This contributes to the total one-way latency value.
    For this reason, network operators try to design their networks to reduce latency as much as possible which is embodied as having as few network
    segments as possible that traffic has to traverse, to investing in high-performant devices with a dedicated chip set for the network function being performed.
    </p>

    <h2>How is Latency measured?</h2>

    <p>
    Latency can be measured in a variety of ways. Fortunately network protocols exist which provide functionality for sending a packet with the designation you
    would like a response. So, just simply measuring the time from sending the first packet to receiving the response will allow you to calculate the time.
     The Internet Control Message Protocol (ICMP) is the most common protocol used for this exercise. ICMP has many different types of
    messages you can send specified by a "Type" field in the ICMP header (part of a packet which specifies details of the associated protocol).
    An ICMP packet with the "Type" set to "8" is an "Echo Request". When a device that is running ICMP receives an ICMP Type 8 packet (Echo Request), the
    protocol specification (code) and calls a function to craft and send an ICMP packet back to the sender with the Type field set to 0, which is also
    known as an "Echo Reply". So, if we measure the time from the Echo Request sent to the time we receive a reply, we can calulate the total round trip latency.
    </p>

    <p>
    This functionality built into ICMP has made it the protocol of choice for measuring round-trip latency between a source and destination. Unfortunately, due
    to concern for performance and security reasons, ICMP is not enabled on every internet device. Forunately, we can use other protocols such as TCP, UDP, and SCTP.
    </p>

    <h2>Common Solutions</h2>

    <p>
    Latency has traditionally been "combated" by improving hardware and reducing distance to a destination. Having resources located physically closer to end users that need to access them. For this reason, content delivery networks (CDNs)
    have been invented to cache content that users are trying to access to reduce the amount of hops, and therefore latency, that is incurred. Every hop in between a user
    and their destination takes time to process the traffic, determine the destination, and then forward it out the approrpriate interface. Even if this takes an additional 250 microseconds
    per hop, that is still additional latency incurred. Both dedicated FPGAs and ASICs in networking devices and optical technologies have made significant advancements and will certainly continue to do so over time.
    </p>

    <h2>Understanding a Traceroute</h2>
    <p>
    Traceroute is a common tool used by network engineers to retrieve round trip times, diagnose latency and loss both to their target and upstream hops,
    and benchmark performance. In your terminal,  if you simply type 'traceroute 8.8.8.8 (One of Google's Public DNS Servers)' you will see something like the following:
    </p>
    <p>
    traceroute to 8.8.8.8 (8.8.8.8), 64 hops max, 52 byte packets<br>
    1  router (10.10.29.1)  85.052 ms  2.168 ms  2.160 ms<br>
    3  173.219.222.228 (173.219.222.228)  111.264 ms  13.597 ms  11.624 ms<br>
    4  173.219.251.218 (173.219.251.218)  44.163 ms  20.716 ms  21.334 ms<br>
    5  142.250.166.156 (142.250.166.156)  71.101 ms  20.346 ms  23.032 ms<br>
    6  * * *<br>
    7  142.251.70.104 (142.251.70.104)  79.207 ms<br>
       dns.google (8.8.8.8)  75.947 ms  22.424 ms<br>
    </p>
    <p>
    Traceroute uses UDP by default on Linux/Unix systems and ICMP on Windows systems. This can be altered, however, by simply specifying the "-P" option
    before typing the IP address in the command followed by the desired protocol (<i>Example:</i> traceroute -P TCP 8.8.8.8 <i>or</i> traceroute -P ICMP 8.8.8.8).
    </p>
    <p>
    When we examine the traceoute above, we note that there is first a number, starting with "1", on the left hand side. This represents the number of nodes,
    or hops, that are traversed to the final target. So, in this example, there are 7 hops between our device and the target (8.8.8.8). Each hop will have three
    integer values (often in milliseconds) to the right of the IP address. These represent three separate measurements of round trip time from our source device
    to that particular hop.
    </p>
    <p>
    If we look at hop 6, we notice no information and just three "*" listed instead. Why is this? If a device does not support, or is explicitly configured, to
    not send a response packet of the protocol we are using, we will have no round-trip data and we will simply see a "*" displayed instead. Since traceroute sends
    three probes by default, we see three "*" displayed. In modern networks, typically it is not uncommon for certain "flavors" of service or infrastructure providers
    to explicity configure their devices to not respond to probes, as it is additional overhead that the device control planes will have to incur as well as posing a
    security risk as often these protols have been misused by bad-actors for certain attacks like a denial of service.
    </p>
    <p>
    Generally I have noticed that ICMP tends to have the most success as a probing protocol to retrieve round time data, but this can be misleading if trying to
    understand performance for application traffic as that is likely using TCP as a transport protocol.
    </p>
    <h2>What can we observe from this data?</h2>


  <a href="index.html">Home</a>
  </body>
</html>
