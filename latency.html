<!DOCTYPE html>
<html>
<head>

<style>
  .parent{
    background-color: hsla(100%, 100%, 0.5);
    height: 200px;
    width: 200px;
  }
  .child{
    background-color: black;
    height: 25%;
    width: 25%;
    transition: transform 1s cubic-bezier(0.14, 0.17, 0.66, 0.64);
  }
  .parent:hover .child{
    transform: translateX(100%);
  }
</style>

</head>

<body>
<img src="latency.png" alt="latency_pic_here" width="800" height="220">
<div class ="parent">
  <div class = "child"></div>
</div>
<p>
Although it may seem like magic, when you send data over a network from one point to another it is not instant.
This is because we are limited to the speed of light, roughly 300 million meters per second. While that is quite fast,
let's pretend we have a router in New York with an optical cable running perfectly to a router in London, which is a total
distance of ~5,567km. The fastest one bit of data in New York, manifested in reality most commonly as light sent through optical fiber,
can travel to the receiving photodiode (diode which produces current which is converted into voltage with a transimpedense amflifer, and then binary)
 of the optic in London is 18.5 ms due to the laws of physics.
</p>
<p>
While this seems very fast (and it is compared to mailing a letter), we do not have perfect point to point links
between all devices, servers, and nodes on the internet. That would not only be physically very messy but logistically
impossible. For that reason, the internet is broken up into various sub-networks controlled by different businesses,
organizations, governments, militaries, and even single individuals who manage and connect their networks to each other,
often through service providers or various infrastructure providers.
</p>

<p>
Since the design of peering multiple networks together was adopted, network traffic often passes through several
networks, called Autonomous Systems (more on this in the BGP section), to reach it's destination. Let's look at an example!
</p>
<p>
Example: Traffic to a SAAS Application<br>

Let's say you are a lucky work-from-home user in Tampa who needs to connect to your Microsoft Sharepoint folder online
to access a Powerpoint you just got told you have to present tomorrow... yay. The network path you will likely
take is out your local home modem/router to your service provider, across any upsteam providers (perhaps to a VPN endpoint configured by your
company or perhaps be split tunneled, bypassing the VPN), and finally into the Autonomous System hosting the application web server. Note that by this point we
have likely already had our traffic routed through four or five different service providers upstream (on the way to target server). Once the
traffic enters Microsoft's AS (Autonomous System), you likely will first hit a security device or two (Firewall, IDS/IPS), which will hand you off to a load balancer, then a front-end proxy server, which then
routes your request to the appropriate web server. All in all, you have likely traveled some 13-15 "hops", which is just slang for separate network segment,
to your target. But here's the catch, it also has to travel back to you!
</p>
<h2>One-Way vs End-to-End Latency</h2>
<p>
Commonly, network operators, enthusiats, deliquents, and administrators misuse the term latency. Before I understood networking, I always
associated latency, jitter, delay, etc. as all the same thing. Network slang has even been birthed out of this confusion with words like,
"lag" and "In reality,
</p>
<p>
Each hop/device takes some time to process your traffic and send it to the appropriate destination. This is known as processing
delay and queing delay. This constributes to the total one-way. For this reason, network operators try to design their networks to
reduce latency as much as possible which is embodied as having as few network segments as possible that traffic has to traverse, to
investing in high-performant devices with a dedicated chip set for the network function being performed.
</p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>

</body>
</html>
